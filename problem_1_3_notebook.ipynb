{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.3: Performance Comparison\n",
    "\n",
    "This notebook compares the performance between training on single device and multiple devices.\n",
    "\n",
    "We will:\n",
    "1. Run single node training (world_size=1, batch_size=64)\n",
    "2. Run double nodes training (world_size=2, batch_size=128)\n",
    "3. Collect and compare:\n",
    "   - Training time (average over epochs from multiple devices)\n",
    "   - Tokens per second (sum throughput from multiple devices, averaged over epochs)\n",
    "4. Visualize the scaling improvement\n",
    "\n",
    "**Note:** We drop the first epoch as warmup before collecting metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "### Two Ways to Complete Problem 1.3:\n",
    "\n",
    "#### Option A: Use This Notebook (Modal Cloud)\n",
    "- Run all cells in this notebook\n",
    "- Requires Modal account and credits\n",
    "- Fully automated end-to-end\n",
    "- Takes ~1-2 hours to complete\n",
    "\n",
    "#### Option B: Use Standalone Script (Recommended)\n",
    "- Run training experiments locally or on PSC:\n",
    "  ```bash\n",
    "  # Run single device\n",
    "  python project/run_data_parallel.py --world_size 1 --batch_size 64\n",
    "  \n",
    "  # Run multi-device\n",
    "  python project/run_data_parallel.py --world_size 2 --batch_size 128\n",
    "  ```\n",
    "- Then analyze results:\n",
    "  ```bash\n",
    "  python problem_1_3_analysis.py --workdir ./workdir\n",
    "  ```\n",
    "- See `PROBLEM_1_3_README.md` for full instructions\n",
    "\n",
    "**Choose one option and proceed accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Modal App\n",
    "\n",
    "We'll use the Modal infrastructure to run distributed training.\n",
    "\n",
    "**Note:** Make sure you have the Modal CLI installed and authenticated:\n",
    "```bash\n",
    "pip install modal\n",
    "modal token new\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modal app and define image with dependencies\n",
    "app = modal.App(\"problem-1-3-training\")\n",
    "\n",
    "image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.12\")\n",
    "    .pip_install(\n",
    "        \"numpy<2\",\n",
    "        \"torch==2.2.0\",\n",
    "        \"datasets<4.0.0\",\n",
    "        \"transformers==4.37.2\",\n",
    "        \"sacrebleu==2.4.0\",\n",
    "        \"tokenizers\",\n",
    "        \"tqdm\",\n",
    "    )\n",
    "    .add_local_python_source(\"project\")\n",
    "    .add_local_python_source(\"data_parallel\")\n",
    ")\n",
    "\n",
    "# Shared volume for outputs\n",
    "volume = modal.Volume.from_name(\"training-workdir-p1-3\", create_if_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function\n",
    "\n",
    "This function runs distributed training using Modal with specified world_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.function(\n",
    "    image=image,\n",
    "    gpu=\"A10G:2\",  # Single container with 2 GPUs\n",
    "    volumes={\"/workdir\": volume},\n",
    "    timeout=3600 * 4,\n",
    "    cpu=16.0,\n",
    "    memory=65536,\n",
    "    serialized=False,\n",
    ")\n",
    "def run_training(\n",
    "    world_size: int = 2,\n",
    "    batch_size: int = 128,\n",
    "    n_epochs: int = 10,\n",
    "    dataset: str = \"bbaaaa/iwslt14-de-en-preprocess\",\n",
    "    model_max_length: int = 128,\n",
    "    learning_rate: float = 1e-4,\n",
    "):\n",
    "    \"\"\"Run distributed training in a single container with multiple GPUs\"\"\"\n",
    "    import torch.multiprocessing as mp\n",
    "    import torch.distributed as dist\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Set spawn method before doing anything with CUDA\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    # Import the training module\n",
    "    import project.run_data_parallel as rdp\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ[\"PYTEST\"] = \"False\"\n",
    "    rdp.PYTEST = False\n",
    "\n",
    "    # Set workdir to Modal volume\n",
    "    os.chdir(\"/workdir\")\n",
    "\n",
    "    # Get the run_dp function\n",
    "    run_dp = rdp.run_dp\n",
    "\n",
    "    processes = []\n",
    "    backend = dist.Backend.NCCL if torch.cuda.is_available() else dist.Backend.GLOO\n",
    "\n",
    "    # Spawn processes just like the original code\n",
    "    for rank in range(world_size):\n",
    "        p = mp.Process(\n",
    "            target=run_dp,\n",
    "            args=(\n",
    "                rank,\n",
    "                world_size,\n",
    "                backend,\n",
    "                dataset,\n",
    "                model_max_length,\n",
    "                n_epochs,\n",
    "                batch_size,\n",
    "                learning_rate,\n",
    "            ),\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    volume.commit()\n",
    "\n",
    "    # Collect metrics from all result files\n",
    "    metrics_by_rank = {}\n",
    "    for rank in range(world_size):\n",
    "        metrics_by_rank[rank] = []\n",
    "        for epoch_idx in range(n_epochs):\n",
    "            filename = f\"/workdir/workdir/rank{rank}_results_epoch{epoch_idx}.json\"\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    metrics_by_rank[rank].append(data)\n",
    "    \n",
    "    volume.commit()\n",
    "\n",
    "    return {\"status\": \"complete\", \"world_size\": world_size, \"batch_size\": batch_size, \"metrics\": metrics_by_rank}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Metric Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_metrics(metrics_by_rank, drop_first_epoch=True):\n",
    "    \"\"\"Compute aggregated metrics across ranks and epochs\n",
    "    \n",
    "    Args:\n",
    "        metrics_by_rank: Dict mapping rank to list of epoch metrics\n",
    "        drop_first_epoch: Whether to drop the first epoch (warmup)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with aggregated metrics\n",
    "    \"\"\"\n",
    "    # Collect all training times and tokens_per_sec\n",
    "    all_training_times = []\n",
    "    all_tokens_per_sec_by_epoch = []\n",
    "    \n",
    "    world_size = len(metrics_by_rank)\n",
    "    # Check if metrics_by_rank is non-empty and contains data before accessing\n",
    "    if not metrics_by_rank or 0 not in metrics_by_rank:\n",
    "        return {\n",
    "            'avg_training_time': 0.0,\n",
    "            'std_training_time': 0.0,\n",
    "            'avg_tokens_per_sec': 0.0,\n",
    "            'std_tokens_per_sec': 0.0,\n",
    "            'world_size': world_size,\n",
    "        }\n",
    "    n_epochs = len(metrics_by_rank[0])\n",
    "    \n",
    "    start_epoch = 1 if drop_first_epoch else 0\n",
    "    \n",
    "    # For each epoch (excluding first if drop_first_epoch)\n",
    "    for epoch_idx in range(start_epoch, n_epochs):\n",
    "        epoch_training_times = []\n",
    "        epoch_tokens_per_sec = []\n",
    "        \n",
    "        # Collect metrics from all ranks for this epoch\n",
    "        for rank in range(world_size):\n",
    "            if epoch_idx < len(metrics_by_rank[rank]):\n",
    "                epoch_data = metrics_by_rank[rank][epoch_idx]\n",
    "                epoch_training_times.append(epoch_data['training_time'])\n",
    "                epoch_tokens_per_sec.append(epoch_data['tokens_per_sec'])\n",
    "        \n",
    "        # Average training time across ranks for this epoch (only if we have data)\n",
    "        if epoch_training_times:\n",
    "            all_training_times.append(np.mean(epoch_training_times))\n",
    "        \n",
    "        # Sum tokens_per_sec across ranks (total throughput) for this epoch (only if we have data)\n",
    "        if epoch_tokens_per_sec:\n",
    "            all_tokens_per_sec_by_epoch.append(np.sum(epoch_tokens_per_sec))\n",
    "    \n",
    "    # Now compute mean and std across epochs\n",
    "    # Check if we have any data to compute statistics\n",
    "    if len(all_training_times) == 0:\n",
    "        return {\n",
    "            'avg_training_time': 0.0,\n",
    "            'std_training_time': 0.0,\n",
    "            'avg_tokens_per_sec': 0.0,\n",
    "            'std_tokens_per_sec': 0.0,\n",
    "            'world_size': world_size,\n",
    "        }\n",
    "    \n",
    "    avg_training_time = np.mean(all_training_times)\n",
    "    std_training_time = np.std(all_training_times)\n",
    "    \n",
    "    avg_tokens_per_sec = np.mean(all_tokens_per_sec_by_epoch)\n",
    "    std_tokens_per_sec = np.std(all_tokens_per_sec_by_epoch)\n",
    "    \n",
    "    return {\n",
    "        'avg_training_time': avg_training_time,\n",
    "        'std_training_time': std_training_time,\n",
    "        'avg_tokens_per_sec': avg_tokens_per_sec,\n",
    "        'std_tokens_per_sec': std_tokens_per_sec,\n",
    "        'world_size': world_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Experiments\n",
    "\n",
    "### Experiment 1: Single Device (world_size=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting single device training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting single device training...\")\n",
    "with app.run():\n",
    "    # Launch remote run and wait for it to complete\n",
    "    result_single_future = run_training.remote(\n",
    "        world_size=1,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "    )\n",
    "    # Retrieve the actual return value from the future with error handling\n",
    "    try:\n",
    "        result_single = result_single_future.get()\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"Error while retrieving single-device result:\")\n",
    "        traceback.print_exc()\n",
    "        # Re-raise to surface the error in the notebook\n",
    "        raise\n",
    "print(f\"Single device training complete\")\n",
    "metrics_single = result_single['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Multiple Devices (world_size=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting multi-device training...\")\n",
    "with app.run():\n",
    "    # Launch remote run and wait for it to complete\n",
    "    result_multi_future = run_training.remote(\n",
    "        world_size=2,\n",
    "        batch_size=128,\n",
    "        n_epochs=10,\n",
    "    )\n",
    "    # Retrieve the actual return value from the future with error handling\n",
    "    try:\n",
    "        result_multi = result_multi_future.get()\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"Error while retrieving multi-device result:\")\n",
    "        traceback.print_exc()\n",
    "        # Re-raise to surface the error in the notebook\n",
    "        raise\n",
    "print(f\"Multi-device training complete\")\n",
    "metrics_multi = result_multi['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Metrics\n",
    "\n",
    "Now we'll collect the metrics from both experiments, excluding the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process single device metrics\n",
    "print(\"Processing single device metrics...\")\n",
    "aggregated_single = compute_aggregated_metrics(metrics_single, drop_first_epoch=True)\n",
    "\n",
    "print(f\"Single Device Metrics:\")\n",
    "print(f\"  Training Time: {aggregated_single['avg_training_time']:.2f} ± {aggregated_single['std_training_time']:.2f} seconds\")\n",
    "print(f\"  Tokens/Second: {aggregated_single['avg_tokens_per_sec']:.2f} ± {aggregated_single['std_tokens_per_sec']:.2f}\")\n",
    "\n",
    "# Process multi-device metrics\n",
    "print(\"\\nProcessing multi-device metrics...\")\n",
    "aggregated_multi = compute_aggregated_metrics(metrics_multi, drop_first_epoch=True)\n",
    "\n",
    "print(f\"Multi-Device Metrics:\")\n",
    "print(f\"  Training Time: {aggregated_multi['avg_training_time']:.2f} ± {aggregated_multi['std_training_time']:.2f} seconds\")\n",
    "print(f\"  Tokens/Second: {aggregated_multi['avg_tokens_per_sec']:.2f} ± {aggregated_multi['std_tokens_per_sec']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Create comparison plots for training time and tokens per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(means, stds, labels, ylabel, title, filename):\n",
    "    \"\"\"Create a bar plot comparing metrics\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    x = np.arange(len(means))\n",
    "    \n",
    "    ax.bar(x, means, yerr=stds,\n",
    "           align='center', alpha=0.7, ecolor='red', capsize=10, width=0.6,\n",
    "           color=['skyblue', 'lightcoral'])\n",
    "    \n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=11)\n",
    "    ax.yaxis.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "        ax.text(i, mean, f'{mean:.2f}\\n±{std:.2f}', \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to submit/figures directory\n",
    "    output_dir = Path('submit/figures')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / filename\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved figure to {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Time comparison\n",
    "plot_comparison(\n",
    "    means=[aggregated_single['avg_training_time'], aggregated_multi['avg_training_time']],\n",
    "    stds=[aggregated_single['std_training_time'], aggregated_multi['std_training_time']],\n",
    "    labels=['Single Device\\n(world_size=1)', 'Multi-Device\\n(world_size=2)'],\n",
    "    ylabel='Training Time (seconds)',\n",
    "    title='Training Time Comparison',\n",
    "    filename='training_time_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Tokens Per Second comparison\n",
    "plot_comparison(\n",
    "    means=[aggregated_single['avg_tokens_per_sec'], aggregated_multi['avg_tokens_per_sec']],\n",
    "    stds=[aggregated_single['std_tokens_per_sec'], aggregated_multi['std_tokens_per_sec']],\n",
    "    labels=['Single Device\\n(world_size=1)', 'Multi-Device\\n(world_size=2)'],\n",
    "    ylabel='Tokens Per Second',\n",
    "    title='Throughput Comparison',\n",
    "    filename='tokens_per_second_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Print a summary of the speedup achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nSingle Device (world_size=1, batch_size=64):\")\n",
    "print(f\"  Average Training Time: {aggregated_single['avg_training_time']:.2f} ± {aggregated_single['std_training_time']:.2f} seconds\")\n",
    "print(f\"  Average Throughput: {aggregated_single['avg_tokens_per_sec']:.2f} ± {aggregated_single['std_tokens_per_sec']:.2f} tokens/sec\")\n",
    "\n",
    "print(f\"\\nMulti-Device (world_size=2, batch_size=128):\")\n",
    "print(f\"  Average Training Time: {aggregated_multi['avg_training_time']:.2f} ± {aggregated_multi['std_training_time']:.2f} seconds\")\n",
    "print(f\"  Average Throughput: {aggregated_multi['avg_tokens_per_sec']:.2f} ± {aggregated_multi['std_tokens_per_sec']:.2f} tokens/sec\")\n",
    "\n",
    "# Calculate speedup (with validation to avoid division by zero)\n",
    "if aggregated_multi['avg_training_time'] > 0 and aggregated_single['avg_tokens_per_sec'] > 0:\n",
    "    time_speedup = aggregated_single['avg_training_time'] / aggregated_multi['avg_training_time']\n",
    "    throughput_speedup = aggregated_multi['avg_tokens_per_sec'] / aggregated_single['avg_tokens_per_sec']\n",
    "    \n",
    "    print(f\"\\nSpeedup:\")\n",
    "    print(f\"  Training Time Speedup: {time_speedup:.2f}x\")\n",
    "    print(f\"  Throughput Speedup: {throughput_speedup:.2f}x\")\n",
    "else:\n",
    "    print(f\"\\nSpeedup: Cannot compute (insufficient data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nFigures saved to: submit/figures/\")\n",
    "print(\"  - training_time_comparison.png\")\n",
    "print(\"  - tokens_per_second_comparison.png\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsys-f25-hw5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
